# âš¡ RAGnarok â€“ The End of AI Hallucinations

<p align="center">
  <b>Production-Ready Document Intelligence System</b><br>
  <i>Retrieval-Augmented Generation with zero-tolerance hallucination prevention</i>
</p>

<p align="center">
  <a href="#-features">âœ¨ Features</a> |
  <a href="#-installation">âš¡ Installation</a> |
  <a href="#-how-it-works">ğŸ§  How It Works</a> |
  <a href="#-technology-stack">ğŸ›  Technology</a> |
  <a href="#-usage">ğŸ“– Usage</a>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/AI-RAG%20System-blue" alt="RAG"/>
  <img src="https://img.shields.io/badge/LLM-Gemma%202B-green" alt="Gemma"/>
  <img src="https://img.shields.io/badge/Embeddings-BGE-orange" alt="BGE"/>
  <img src="https://img.shields.io/badge/Vector%20DB-FAISS-red" alt="FAISS"/>
  <img src="https://img.shields.io/badge/UI-Streamlit-purple" alt="Streamlit"/>
  <img src="https://img.shields.io/badge/Deploy-Docker-blue" alt="Docker"/>
</p>

---

## ğŸ§ About

**RAGnarok** is a production-ready **Retrieval-Augmented Generation (RAG)** system that brings the end of AI hallucinations in document-based question answering. The system ensures reliable, source-attributed responses by:

- **Grounding answers in provided documents** - Eliminates hallucination
- **Providing complete source attribution** - Every answer includes document references
- **Implementing multi-layer validation** - Built-in guardrails ensure reliability
- **Operating completely offline** - Your data stays private and secure

Ideal for knowledge management, research, document analysis, and any scenario requiring verifiable AI-powered insights from document collections.

---

## âœ¨ Features

### ğŸ¯ **Intelligent Document Processing**

- **Multi-format support**: PDF, TXT, and Markdown files
- **Smart text chunking**: Preserves context with sentence-boundary detection
- **Automatic metadata extraction**: Tracks sources and document structure

### ğŸ” **Advanced Semantic Search**

- **Dense vector embeddings**: Uses state-of-the-art BGE models
- **Cosine similarity matching**: Finds semantically related content, not just keywords
- **Configurable retrieval**: Adjustable relevance thresholds and result counts

### ğŸ§  **Context-Aware AI Responses**

- **Grounded answers**: Responses based only on retrieved document context
- **Source citations**: Every answer includes document references
- **Confidence scoring**: Transparent relevance metrics for each response

### ğŸ›¡ï¸ **Reliability & Safety**

- **Hallucination prevention**: Multi-layer validation prevents made-up information
- **Confidence thresholding**: Refuses to answer when documents lack relevant information
- **Explicit uncertainty**: Clearly states when information is not available

### ğŸ–¥ï¸ **User-Friendly Interface**

- **Clean Streamlit UI**: Intuitive web interface for easy interaction
- **Real-time processing**: Instant document loading and query responses
- **Transparent results**: Shows retrieved contexts and confidence scores

---

## ğŸ–¼ï¸ Application Screenshots

<p align="center">
  <img src="screenshots/home.png" alt="RAGnarok Home Screen" width="85%"/>
</p>

<p align="center">
  <b>Document Loading & System Configuration</b>
</p>

<p align="center">
  <img src="screenshots/answer.png" alt="RAGnarok Answer Screen" width="85%"/>
</p>

<p align="center">
  <b>Intelligent Question Answering with Source Attribution</b>
</p>

---

## âš¡ Installation

### Prerequisites

- Python 3.11 or higher
- Docker (for Ollama LLM service)
- 4GB+ RAM recommended

### Setup Instructions

1. **Clone the repository**

```bash
git clone https://github.com/yourusername/ragnarok.git
cd ragnarok
```

2. **Create virtual environment**

```bash
python -m venv venv

# Windows
venv\Scripts\activate

# macOS/Linux
source venv/bin/activate
```

3. **Install dependencies**

```bash
pip install -r requirements.txt
```

4. **Setup Ollama with Docker**

```bash
# Pull and run Ollama container
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama

# Pull the Gemma 2B model
docker exec -it ollama ollama pull gemma:2b

# Verify the setup
curl http://localhost:11434/api/tags
```

5. **Add your documents**

```bash
# The documents folder already exists with sample content
# Add your PDF, TXT, or Markdown files to documents/
```

6. **Process documents and run the application**

````bash
# Process documents (one-time setup)
python main.py setup

# Run the web interface
streamlit run app.py
Visit `http://localhost:8501` to start using RAGnarok.

### ğŸ³ Docker Troubleshooting

**Check if Ollama container is running:**
```bash
docker ps | grep ollama
````

**View container logs:**

```bash
docker logs ollama
```

**Restart Ollama container:**

```bash
docker restart ollama
```

**Test Ollama API:**

```bash
curl http://localhost:11434/api/tags
```

---

## ğŸ§  How It Works

### The RAG Process Explained

```mermaid
flowchart TD
    A[ğŸ“„ Your Documents] --> B[ğŸ”§ Text Processing]
    B --> C[âœ‚ï¸ Smart Chunking]
    C --> D[ğŸ§® Generate Embeddings]
    D --> E[ğŸ—„ï¸ Vector Database]

    F[â“ Your Question] --> G[ğŸ§® Query Embedding]
    G --> H[ğŸ” Similarity Search]
    E --> H
    H --> I[ğŸ“‹ Retrieved Contexts]
    I --> J[ğŸ¤– AI Response Generation]
    J --> K[ğŸ›¡ï¸ Guardrails Check]
    K --> L[âœ… Final Answer + Sources]
```

### Step-by-Step Process:

1. **Document Ingestion**: Your documents are processed and split into meaningful chunks
2. **Embedding Generation**: Each chunk is converted to a high-dimensional vector using BGE models
3. **Vector Storage**: Embeddings are stored in a FAISS database for fast similarity search
4. **Query Processing**: Your question is converted to the same vector space
5. **Semantic Retrieval**: Most relevant document chunks are found using cosine similarity
6. **Context Assembly**: Retrieved chunks are formatted into a coherent context
7. **AI Generation**: Llama3 generates an answer based strictly on the provided context
8. **Validation**: Guardrails ensure the response is grounded and reliable

---

## ğŸ› ï¸ Technology Stack

### **Core AI Components**

- **ğŸ§  Large Language Model**: Gemma 2B via Ollama Docker (local deployment)
- **ğŸ”¤ Embeddings**: BAAI/bge-base-en-v1.5 (state-of-the-art retrieval model)
- **ğŸ—„ï¸ Vector Database**: FAISS (Facebook AI Similarity Search)

### **Why Gemma 2B?**

- **Efficiency**: Smaller model size (2B parameters) for faster inference
- **Quality**: Google's latest architecture with strong reasoning capabilities
- **Resource Friendly**: Lower memory requirements (4GB vs 8GB+ for larger models)
- **Docker Optimized**: Excellent performance in containerized environments

### **Document Processing**

- **ğŸ“„ PDF Processing**: PyPDF2 for text extraction
- **ğŸ“ Text Processing**: Native Python with smart chunking
- **ğŸ“‹ Markdown Support**: Python-markdown for conversion

### **Application Framework**

- **ğŸ–¥ï¸ User Interface**: Streamlit for interactive web UI
- **ğŸ Backend**: Python 3.11+ with async support
- **âš™ï¸ Configuration**: TOML-based project configuration

### **Safety & Reliability**

- **ğŸ›¡ï¸ Guardrails**: Custom validation system
- **ğŸ“Š Confidence Scoring**: Similarity-based relevance metrics
- **ğŸ” Source Attribution**: Automatic citation generation

---

## ğŸ“– Usage

### Web Interface

1. **Load Documents**: Click "Load / Reload Documents" to process files from the `documents/` folder
2. **Ask Questions**: Enter your question in the text area
3. **Get Answers**: Receive grounded responses with source citations and confidence scores

### Command Line Interface

```bash
# Process documents
python main.py setup --documents-dir ./documents

# Query from command line
python main.py query "What are the main topics discussed in the documents?"
```

### Programmatic Usage

```python
from rag_pipeline import RAGPipeline
from document_processor import DocumentProcessor

# Initialize system
processor = DocumentProcessor()
pipeline = RAGPipeline()

# Process documents
chunks = processor.process_directory("documents")
pipeline.add_documents(chunks)

# Query
result = pipeline.generate_response("Your question here")
print(result['response'])
print("Sources:", result['sources'])
```

---

## ğŸ¯ Use Cases

### **Enterprise & Business**

- **Knowledge Management**: Query company documentation, policies, and procedures
- **Legal Research**: Search through contracts, regulations, and compliance documents
- **Technical Documentation**: Navigate complex manuals and specifications
- **Business Intelligence**: Analyze reports and extract key insights

### **Research & Academia**

- **Literature Review**: Query research papers and academic publications
- **Study Materials**: Get answers from textbooks and course materials
- **Data Analysis**: Extract insights from research datasets and reports
- **Thesis Research**: Analyze large collections of academic sources

### **Personal & Professional Development**

- **Document Organization**: Manage and query personal knowledge bases
- **Learning**: Interactive study sessions with educational materials
- **Project Research**: Access relevant information from document collections
- **Decision Support**: Evidence-based insights from document analysis

---

## ğŸ“‚ Project Structure

```
ragnarok/
â”œâ”€â”€ ğŸ“± app.py                    # Streamlit web interface
â”œâ”€â”€ ğŸ”§ rag_pipeline.py           # Core RAG orchestration
â”œâ”€â”€ ğŸ“„ document_processor.py     # Document ingestion & chunking
â”œâ”€â”€ ğŸ§® embeddings.py             # Embedding generation
â”œâ”€â”€ ğŸ—„ï¸ vector_store.py           # FAISS vector database
â”œâ”€â”€ ğŸ›¡ï¸ guardrails.py             # Safety & validation system
â”œâ”€â”€ ğŸ–¥ï¸ main.py                   # CLI interface
â”œâ”€â”€ ğŸ“‹ requirements.txt          # Python dependencies
â”œâ”€â”€ âš™ï¸ pyproject.toml            # Project configuration
â”œâ”€â”€ ğŸ“ documents/                # Your input documents
â”œâ”€â”€ ğŸ—ƒï¸ vector_store/             # Generated vector database
â”œâ”€â”€ ğŸ“¸ screenshots/              # Application screenshots
â”œâ”€â”€ ğŸ“– README.md                 # This file
â””â”€â”€ ğŸ“š TECHNICAL_DOCUMENTATION.md # Detailed technical guide
```

---

## ğŸ”§ Configuration

### System Parameters

The system uses the following default configuration:

```python
# Embedding Model
EMBEDDING_MODEL = "BAAI/bge-base-en-v1.5"

# LLM Model (requires Ollama Docker)
LLM_MODEL = "gemma:2b"

# Retrieval Settings
TOP_K_RESULTS = 5              # Number of contexts to retrieve
MIN_CONFIDENCE = 0.5           # Minimum similarity threshold

# Document Processing
CHUNK_SIZE = 512               # Characters per chunk
CHUNK_OVERLAP = 50             # Overlap between chunks
```

### Supported Document Formats

- **PDF**: `.pdf` files using PyPDF2
- **Text**: `.txt` files with UTF-8 encoding
- **Markdown**: `.md` and `.markdown` files

---

## ğŸš€ Performance

### System Requirements

- **Minimum**: 4GB RAM, 2GB storage
- **Recommended**: 8GB RAM, 5GB storage, SSD
- **Optimal**: 16GB RAM, GPU acceleration

### Performance Characteristics

- **Document Processing**: ~1000 pages/minute
- **Query Response**: 1-3 seconds (Gemma 2B optimized)
- **Document Capacity**: 10,000+ documents supported
- **Concurrent Users**: 5-10 (Docker deployment)
- **Memory Usage**: 4GB RAM (efficient Gemma 2B model)

---

## ğŸ”’ Privacy & Security

- **Local Processing**: All operations happen on your machine
- **No External APIs**: No data sent to third-party services
- **Secure Storage**: Documents and embeddings stored locally
- **Access Control**: File system permissions protect your data

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.

### Development Setup

```bash
git clone https://github.com/DreamerAkhilesh/RAGnarok.git
cd RAGnarok
pip install -r requirements.txt
pip install -e .
```

### Running Tests

```bash
python -m pytest tests/
```

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- [Hugging Face](https://huggingface.co/) for Sentence Transformers
- [Meta](https://ai.meta.com/) for the Llama3 language model
- [Facebook AI](https://ai.facebook.com/) for FAISS vector search
- [Streamlit](https://streamlit.io/) for the web interface framework
- [Ollama](https://ollama.ai/) for local LLM deployment

---

<p align="center">
  <b>RAGnarok - The End of AI Hallucinations</b><br>
  <i>Production-ready document intelligence with zero-tolerance hallucination prevention</i>
</p>
